STEP 9 

R Code for GBM Model
		
*************************************************************************************************
##Reading Data
```{r}
options(max.print=999999999)
options(warn=-1)
library(caret)
library(readstata13)

data2<-read.dta13(file = "file path")


# looking at the dimensions of the dataset
dim(data2)

# looking at any missing values in our dataset
sum(is.na(data2))

# Converting all the variables of the dataset into categorical variables 
data2[c(1:47)] <- lapply(data2[,c(1:47)] , factor)
data2$DISCHARGES <- as.numeric(data2$DISCHARGES)

# taking a look at the structure of the varaibles in the dataset
str(data2[1:20])
str(data2[21:50])


data2$mortality <- factor(data2$mortality,levels = c(0,1),
labels <- c("NO", "YES"))

#tabulating the outcome variable
print(table(data2$mortality))

print(prop.table(table(data2$mortality)))


outcomeName <- 'mortality'

# Splitting the dataset into Training and testing datasets to train and test the model performance
# split data2 into a train and a test set; train, test, and model. The train and test split is 70/30
set.seed(1234)
splitIndex <- createDataPartition(data2[,outcomeName], p = .70, list = FALSE, times = 1)
train_data <- data2[ splitIndex,]
test_data  <- data2[-splitIndex,]


nrow(train_data)
# 364337 observations pertain to the training data set.

nrow(test_data)
# 156143 observations pertain to the testing/evaluation data set.

#tabulating the outcome variable for overall, training and testing data

print(table(data2$mortality))

print(prop.table(table(data2$mortality)))

print(prop.table(table(train_data$mortality)))
print(prop.table(table(test_data$mortality)))


```
## Running a Gradiant Boosted  Classfication Model
```{r}
options(warn=-1)
library(caret)
library(ROCR)
library(pROC)
library(xgboost)
library(gbm)

set.seed(1234)

gbm.Control <- trainControl(method='cv', number=10, returnResamp='none', summaryFunction = twoClassSummary, classProbs = TRUE)

gbm <- train(mortality ~ ADMSRC + MI + CHF + PVD + STROKE + LIVERMILD + DM + DMCX + RENAL + CANCER + METS + ASTHMA + 
PLEURAL_EFFUSION + RESPIRATORY_FAILURE + PHLEBITIS_THROMBOPHLEBITIS + CARDIAC_ARRHYTHMIA + SEPSIS + URINARY_TRACT + MALNUTRITION + OTHER_FLUID_DISORDERS + SEVERE_STRESS + ACUTE_RENAL_FAILURE + CCIcat + Aweekend + MOD_CODE + ETHNICITY + RACE + emergencyservices + Hospital_Rating + Comp_SafetyofCare + Comp_Readmission + Comp_CareEffectiveness + Comp_ImagingEffectivenss + Age_Cat + LOS + DISCHARGES, data = train_data, 

                  method='gbm', 

                  trControl=gbm.Control,  

                  metric = "ROC",

                  preProc = c("center", "scale"))

#Looking at relative influence of each predictor to model

summary(gbm)

gbm

# Evaluating the model performance on the test data set
pred.gbm <- predict(gbm, newdata = test_data)

# Confusion matrix for the random forest model 
cm.gbm<-confusionMatrix(table(pred.gbm, test_data$mortality)) ; cm.gbm

# List of variable importance
(varImp(gbm,scale=F))


library(pROC)
gbm.Predict.2 <- predict(gbm,newdata = test_data , type="prob")
gbm.ROC.2 <- roc(test_data$mortality,gbm.Predict.2[,"YES"], ci = T, auc = T) ;gbm.ROC.2

# Making a ROC plot for the model using
gbm.Predict2 <- prediction(predict(gbm, newdata = test_data, type = "prob")[,2],test_data$mortality)

ROC.plot.data<-plot(performance(gbm.Predict2, "tpr","fpr"), xlab = "FPR (1 - Specificity)", ylab = "TPR (Sensitivity)", main = "Elective Procedures Mortality  GBM", col=rainbow(7))
text(0.5,0.5,paste("AUC = ",format(gbm.ROC.2$auc, digits=5, scientific=FALSE)))
abline(0,1, lty = 2)


```

